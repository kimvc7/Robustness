{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "juvenile-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"uci_all\"\n",
    "results_dir = './results/' + experiment_name + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "renewable-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36064 config files created\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "    \n",
    "import runs.config_experiments_uci_all as run\n",
    "experiment_list = run.config_experiments(results_dir, create_json=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "certified-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./results/datasets/UCI/datasets_info.csv'):\n",
    "    import json\n",
    "    datasets = []\n",
    "    for data_set in range(20,66):\n",
    "        with open('/vast/robustness/configs_datasets/' + str(data_set) + '.json') as config_file:\n",
    "            config = json.load(config_file)\n",
    "            tmpX = np.shape(np.genfromtxt('/vast/robustness/datasets/UCI/imp_' + config[\"name_file\"] \n",
    "                                 + '_' + \"trainX.csv\", delimiter=','))\n",
    "            config[\"num_examples\"] = tmpX[0]\n",
    "            config[\"num_features\"] = tmpX[1]\n",
    "            datasets.append(config)\n",
    "    df = pd.DataFrame(datasets) \n",
    "    df.to_csv('./results/datasets/UCI/datasets_info.csv')\n",
    "else:\n",
    "    df = pd.read_csv('./results/datasets/UCI/datasets_info.csv')\n",
    "\n",
    "name_attacks = [\"l1_pgd_norm\", \"l1_fgm_norm\",\"l2_pgd_norm\", \"l2_fgm_norm\", \"linf_pgd\",\"l2_pgd\", \"linf_fgsm\", \"l2_fgm\"]#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sticky-superior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>name_file</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>num_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>UCI</td>\n",
       "      <td>ozone-level-detection-eight</td>\n",
       "      <td>2</td>\n",
       "      <td>2027</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>UCI</td>\n",
       "      <td>optical-recognition-handwritten-digits</td>\n",
       "      <td>10</td>\n",
       "      <td>3058</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>UCI</td>\n",
       "      <td>spambase</td>\n",
       "      <td>2</td>\n",
       "      <td>3681</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>36</td>\n",
       "      <td>UCI</td>\n",
       "      <td>ozone-level-detection-one</td>\n",
       "      <td>2</td>\n",
       "      <td>2029</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>61</td>\n",
       "      <td>UCI</td>\n",
       "      <td>cnae-9</td>\n",
       "      <td>9</td>\n",
       "      <td>864</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  dataset_id dataset_name  \\\n",
       "0            0          20          UCI   \n",
       "4            4          24          UCI   \n",
       "15          15          35          UCI   \n",
       "16          16          36          UCI   \n",
       "41          41          61          UCI   \n",
       "\n",
       "                                 name_file  num_classes  num_examples  \\\n",
       "0              ozone-level-detection-eight            2          2027   \n",
       "4   optical-recognition-handwritten-digits           10          3058   \n",
       "15                                spambase            2          3681   \n",
       "16               ozone-level-detection-one            2          2029   \n",
       "41                                  cnae-9            9           864   \n",
       "\n",
       "    num_features  \n",
       "0             73  \n",
       "4             65  \n",
       "15            58  \n",
       "16            73  \n",
       "41           857  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.num_features>50) & (df.num_examples > 500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "annoying-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "allied-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "approxs = ['linf', 'l1'] \n",
    "test = {}\n",
    "for approx in approxs:\n",
    "    file_name = results_dir + experiment_list[1]['model_name'] + '/results/acc_' + 'test' + '_approx_bound_' + approx + '.pkl'\n",
    "    if not os.path.isfile(file_name):\n",
    "        print(\"Missing!! \" + file_name)\n",
    "        continue\n",
    "\n",
    "    with open(file_name, 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "            \n",
    "    test[approx] = {}\n",
    "    for epsilon in tmp['bound'].keys():\n",
    "        test[approx][epsilon] = {}\n",
    "        test[approx][epsilon]['test'] = 0\n",
    "        test[approx][epsilon]['total'] = 0\n",
    "        \n",
    "for idx, _ in enumerate(experiment_list):\n",
    "    for approx in approxs:\n",
    "        file_name = results_dir + experiment_list[idx]['model_name'] + '/results/acc_' + 'test' + '_approx_bound_' + approx + '.pkl'\n",
    "        if not os.path.isfile(file_name):\n",
    "            print(\"Missing!! \" + file_name)\n",
    "            continue\n",
    "\n",
    "        with open(file_name, 'rb') as f:\n",
    "            tmp = pickle.load(f)\n",
    "            \n",
    "        for epsilon in tmp['bound'].keys():\n",
    "            if np.isnan(tmp['bound'][epsilon]) or np.isnan(tmp['xent'][epsilon]):\n",
    "                continue\n",
    "            test[approx][epsilon]['total'] += 1\n",
    "            if tmp['bound'][epsilon] >= tmp['xent'][epsilon]:\n",
    "                test[approx][epsilon]['test'] += 1\n",
    "for approx in approxs:\n",
    "    for epsilon in tmp['bound'].keys():\n",
    "        test[approx][epsilon] = test[approx][epsilon]['test'] /  test[approx][epsilon]['total']     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "horizontal-actress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0.0000    0.0002    0.0005    0.0008    0.0010    0.0015    0.0020  \\\n",
      "linf  0.935609  0.986844  0.987781  0.987490  0.988072  0.987846  0.986941   \n",
      "l1    0.934030  0.989749  0.990493  0.990816  0.990945  0.990978  0.991042   \n",
      "\n",
      "        0.0030    0.0100    0.1000    0.3000    0.5000    1.0000  \n",
      "linf  0.987296  0.984322  0.948991  0.862199  0.803142  0.786527  \n",
      "l1    0.991366  0.991818  0.990816  0.989458  0.987162  0.981891  \n"
     ]
    }
   ],
   "source": [
    "df_approx = pd.DataFrame.from_dict(test, orient='index')\n",
    "df_approx.to_csv('bound_approx.csv', index=False)\n",
    "print(df_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "shaped-retention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:234: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:195: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_entries = []\n",
    "for net in ['OneLayer', 'ThreeLayer']:\n",
    "    for dataset_id in list(df.dataset_id):\n",
    "\n",
    "        for idx, exp in enumerate(experiment_list):\n",
    "            if exp['data_set'] == dataset_id: \n",
    "                break\n",
    "                \n",
    "        for attack in name_attacks:\n",
    "\n",
    "            file_name = results_dir + experiment_list[idx]['model_name'] + '/results/acc_' + 'val' + '_' + attack + '.pkl'\n",
    "            if not os.path.isfile(file_name):\n",
    "                print(\"Missing!! \" + file_name)\n",
    "                continue\n",
    "            with open(file_name, 'rb') as f:\n",
    "                tmp = pickle.load(f)\n",
    "\n",
    "\n",
    "            for cv_epsilon in list(tmp.keys()):\n",
    "                # Hash table of parameters\n",
    "                parameters = {\"epsilon\": {}, \"backbone\": {}, \"initial_learning_rate\": {}, \n",
    "                              \"robust_training\": {}, \"type_robust\": {}, \"epsilon_pgd_training\":{}}\n",
    "\n",
    "                to_exclude = []\n",
    "                experiment_list_tmp = [element for i, element in enumerate(experiment_list) if i not in to_exclude]\n",
    "                for exp in experiment_list_tmp:\n",
    "                    if not exp['data_set'] == dataset_id: \n",
    "                        continue\n",
    "                    for kk in parameters.keys():\n",
    "                        if exp[kk] in parameters[kk]:\n",
    "                            parameters[kk][exp[kk]].append(int(exp[\"model_name\"]))\n",
    "                        else:\n",
    "                            parameters[kk][exp[kk]] = [int(exp[\"model_name\"])]\n",
    "\n",
    "\n",
    "                # For all methods, do cross-val and create an entry of the results\n",
    "                backbones = [net, net + '+pgd']\n",
    "\n",
    "\n",
    "\n",
    "                for backbone in backbones:\n",
    "                    for robust_training in [True, False]:\n",
    "                        if robust_training:\n",
    "                            type_robust_trainings = ['l1','linf', \"certificate\", \"grad\"] \n",
    "                        else:\n",
    "                            type_robust_trainings = ['none']\n",
    "                        for type_robust in type_robust_trainings:\n",
    "\n",
    "                            if (backbone == 'Madry' and robust_training == True) or \\\n",
    "                                (backbone == 'CNN+clipping' and robust_training == False):\n",
    "                                continue\n",
    "\n",
    "                            if robust_training==False:\n",
    "                                ids = list(set(parameters[\"backbone\"][backbone]) & \n",
    "                                           set(parameters[\"robust_training\"][False]))\n",
    "                            else:\n",
    "                                ids = list(set(parameters[\"backbone\"][backbone]) & \n",
    "                                       set(parameters[\"robust_training\"][True])&\n",
    "                                      set(parameters[\"type_robust\"][type_robust]))\n",
    "\n",
    "\n",
    "\n",
    "                            if backbone == net + '+pgd' and robust_training == True:\n",
    "                                continue\n",
    "\n",
    "                            if ids == []:\n",
    "                                continue\n",
    "                            #print(ids)\n",
    "\n",
    "\n",
    "                            # Cross-validation among learning rates and epsilons:\n",
    "                            best_acc = -1\n",
    "                            best_id = ids[0]\n",
    "                            for id in ids:\n",
    "                                file_name = results_dir + experiment_list[id]['model_name'] + '/results/acc_' + 'val' + '_' + attack + '.pkl'\n",
    "                                if not os.path.isfile(file_name):\n",
    "                                    print(\"Missing!! \" + file_name)\n",
    "                                    continue\n",
    "\n",
    "                                with open(file_name, 'rb') as f:\n",
    "                                    tmp = pickle.load(f)\n",
    "\n",
    "                                acc = tmp[cv_epsilon]\n",
    "                                if acc>best_acc:\n",
    "                                    best_id = id\n",
    "                                    best_acc = acc\n",
    "\n",
    "                            if best_acc == -1:\n",
    "                                continue\n",
    "\n",
    "                            if (robust_training == False) & (backbone==net):  \n",
    "                                name_legend = 'vanilla'\n",
    "                            elif backbone== net + '+pgd':\n",
    "                                name_legend = 'pgd'\n",
    "                            else:\n",
    "                                if type_robust=='certificate':\n",
    "                                    name_legend = 'RUB'\n",
    "                                elif type_robust=='linf':\n",
    "                                    name_legend = 'aRUB_Linf'\n",
    "                                elif type_robust=='grad':\n",
    "                                    name_legend = 'grad'\n",
    "                                else:\n",
    "                                    name_legend = 'aRUB_L1'\n",
    "\n",
    "                            entry = {\"dataset\": dataset_id,\n",
    "                                     \"net\": net,\n",
    "                                    \"learning_rate\": experiment_list[best_id]['initial_learning_rate'],\n",
    "                                    \"robust_training\": name_legend,\n",
    "                                    \"epsilon\": experiment_list[best_id]['epsilon'],\n",
    "                                    \"epsilon_pgd_training\": experiment_list[best_id]['epsilon_pgd_training']}\n",
    "\n",
    "                            dataset = \"test\"\n",
    "                            entry[\"attack\"] = attack\n",
    "                            entry[\"experiment_id\"] = best_id\n",
    "\n",
    "                            with open(results_dir + experiment_list[best_id]['model_name'] + '/results/acc_' + dataset + '_' + \n",
    "                                attack + '.pkl', 'rb') as f:\n",
    "                                tmp = pickle.load(f)\n",
    "\n",
    "                            entry[\"test_epsilon\"] = cv_epsilon\n",
    "                            entry[\"accuracy\"] =  100*tmp[cv_epsilon]\n",
    "                            \n",
    "                            with open(results_dir + experiment_list[best_id]['model_name'] + \n",
    "                                          '/results/training_time.pkl', 'rb') as f:\n",
    "                                tmp = pickle.load(f)\n",
    "                                entry[\"images_per_second\"] = np.mean(tmp) \n",
    "                                entry[\"std_images_per_second\"] = np.std(tmp)\n",
    "                            \n",
    "                            list_entries.append(entry.copy())\n",
    "\n",
    "df_results = pd.DataFrame.from_dict(list_entries) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caring-burst",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results.to_csv('uci_norm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}